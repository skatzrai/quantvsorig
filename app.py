import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional

import gdown
import huggingface_hub
import numpy as np
import psutil
import pandas as pd
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

st.set_page_config(page_title="Quantized model tester", layout="wide")

# ============================================================
# üî• QuantModel (–≤—Å—Ç—Ä–æ–µ–Ω —Å—é–¥–∞, —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –ø—É–ª–ª–∏–Ω–≥–æ–º —Ç–æ–∫–µ–Ω–æ–≤)
# ============================================================
class QuantModel:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π."""
    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: Optional[str] = None,
                 force_download: bool = False):
        self.model_id = model_id
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name
        self.force_download = force_download
        self.model_path = None

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = self._load_tokenizer()

    def _ensure_model(self):
        os.makedirs(self.model_dir, exist_ok=True)
        need_download = self.force_download or not any(self.model_dir.glob("*.onnx"))

        if need_download:
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                os.remove(zip_path)
            elif self.source == "hf":
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False,
                    resume_download=True
                )
            elif self.source == "local":
                # –Ω–∏—á–µ–≥–æ –Ω–µ –∫–∞—á–∞–µ–º ‚Äî –±–µ—Ä–µ–º –∫–∞–∫ –µ—Å—Ç—å
                pass
            else:
                raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {self.source}")

        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –ù–µ—Ç .onnx –º–æ–¥–µ–ª–∏ –≤ {self.model_dir}")
        self.model_path = onnx_files[0]

    def _load_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    def _load_tokenizer(self):
        if self.tokenizer_name:
            return AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)
        try:
            return AutoTokenizer.from_pretrained(str(self.model_dir), use_fast=True)
        except Exception:
            return AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3", use_fast=True)

    @lru_cache(maxsize=1024)
    def _encode_cached(self, text: str, normalize: bool = True):
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        # –ü—Ä–æ–≥–æ–Ω
        outputs = self.session.run(None, ort_inputs)
        embeddings = outputs[0]

        # –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º—ã:
        # - –µ—Å–ª–∏ (batch, seq, dim) -> –¥–µ–ª–∞–µ–º masked mean –ø–æ seq (attention_mask)
        # - –µ—Å–ª–∏ (batch, dim) -> —É–∂–µ sentence embedding
        if embeddings.ndim == 3:
            # masked mean pooling
            mask = ort_inputs.get("attention_mask", None)
            if mask is not None:
                mask = mask.astype(np.float32)[..., None]  # (batch, seq, 1)
                summed = (embeddings * mask).sum(axis=1)   # (batch, dim)
                counts = mask.sum(axis=1)                  # (batch, 1)
                counts = np.clip(counts, 1e-6, None)
                embeddings = summed / counts
            else:
                embeddings = embeddings.mean(axis=1)

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º (dim,) –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return embeddings[0]

    def encode(self, texts, normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_cached(t, normalize) for t in texts])


# ============================================================
# üîß Helpers
# ============================================================
def to_vector(embs):
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –±–∞—Ç—á —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ)."""
    arr = np.array(embs)
    # (dim,)
    if arr.ndim == 1:
        return arr
    # (1, dim)
    if arr.ndim == 2 and arr.shape[0] == 1:
        return arr[0]
    # (batch, seq, dim) -> —Å—Ä–µ–¥–Ω–µ–µ –ø–æ seq, –∑–∞—Ç–µ–º –ø–æ batch
    if arr.ndim == 3:
        arr = arr.mean(axis=1)
    # (batch, dim) -> —Å—Ä–µ–¥–Ω–µ–µ –ø–æ batch
    return arr.mean(axis=0)


def cosine_similarity(vec1, vec2):
    return float(np.dot(vec1, vec2) / ((norm(vec1) * norm(vec2)) + 1e-12))


def cosine_batch(A, B):
    """–ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω—ã–π –∫–æ—Å–∏–Ω—É—Å –¥–ª—è –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."""
    A = np.asarray(A)
    B = np.asarray(B)
    if A.shape != B.shape:
        # –ø–æ–¥—Ä–µ–∑–∞–µ–º –¥–æ min(dim)
        m = min(A.shape[-1], B.shape[-1])
        A = A[..., :m]
        B = B[..., :m]
    # —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã; –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø–æ–≤—Ç–æ—Ä–∏–º
    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)
    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)
    return (A * B).sum(axis=1)


# ============================================================
# üéõÔ∏è UI
# ============================================================
st.title("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –û—Ä–∏–≥–∏–Ω–∞–ª vs –ö–≤–∞–Ω—Ç")

mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å", "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å", "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö"])

# –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞ —Å–µ—Å—Å–∏–∏/–∫—ç—à–µ–π
if st.button("‚ôªÔ∏è –°–±—Ä–æ—Å–∏—Ç—å —Å–µ—Å—Å–∏—é"):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.rerun()

input_text = st.text_area("–¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)", "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.\n–ü—Ä–∏–º–µ—Ä –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏.")
texts = [t.strip() for t in input_text.split("\n") if t.strip()]
batch_size = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è throughput-—Ç–µ—Å—Ç–∞", 1, 128, 8)
force_download = st.checkbox("‚ôªÔ∏è –ü–µ—Ä–µ–∫–∞—á–∞—Ç—å –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ", False)

metrics_df = None

if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
    model_id = st.text_input("HF repo ID", "deepvk/USER-BGE-M3")

elif mode == "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å":
    col1, col2 = st.columns(2)
    with col1:
        quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫", ["gdrive", "hf", "local"], index=1)
        quant_id = st.text_input("ID/Repo/Path", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    with col2:
        quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞", "onnx-user-bge-m3")
        tokenizer_name = st.text_input("Tokenizer name", "")

else:  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö
    st.markdown("–í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –∏–∑–º–µ—Ä—è–µ–º **—Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–æ** (cosine similarity) –∏ —Å–∫–æ—Ä–æ—Å—Ç—å. –ü–∞–º—è—Ç—å –Ω–µ –º–µ—Ä—è–µ–º.")
    col1, col2 = st.columns(2)
    with col1:
        model_id = st.text_input("HF repo ID (–æ—Ä–∏–≥–∏–Ω–∞–ª)", "deepvk/USER-BGE-M3", key="orig_repo_cmp")
    with col2:
        quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–≤–∞–Ω—Ç–∞", ["gdrive", "hf", "local"], index=1, key="quant_src_cmp")
        quant_id = st.text_input("ID/Repo/Path (–∫–≤–∞–Ω—Ç)", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36", key="quant_id_cmp")
    col3, col4 = st.columns(2)
    with col3:
        quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞", "onnx-user-bge-m3", key="quant_dir_cmp")
    with col4:
        tokenizer_name = st.text_input("Tokenizer name", "", key="tok_cmp")

run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

# ============================================================
# üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
# ============================================================
if run_button:
    try:
        texts_for_run = (texts * batch_size)[:max(len(texts), 1)]

        if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
            proc = psutil.Process()
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = SentenceTransformer(model_id)
            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize_embeddings=True)
            t1 = time.perf_counter()
            latency = t1 - t0
            memory = proc.memory_info().rss / 1024 ** 2

            metrics_df = pd.DataFrame([{
                "Mode": "Original",
                "Batch Size": batch_size,
                "Latency (s)": latency,
                "Throughput (texts/s)": len(texts_for_run) / max(latency, 1e-12),
                "Memory (MB)": memory
            }])

            st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            st.metric("Latency (s)", f"{latency:.4f}")
            st.metric("Throughput (texts/s)", f"{len(texts_for_run)/max(latency,1e-12):.1f}")
            st.metric("Memory (MB)", f"{memory:.1f}")
            st.dataframe(metrics_df)

        elif mode == "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å":
            proc = psutil.Process()
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = QuantModel(
                    model_id=quant_id,
                    source=quant_source,
                    model_dir=quant_dir,
                    tokenizer_name=tokenizer_name if tokenizer_name else None,
                    force_download=force_download
                )
            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize=True)
            t1 = time.perf_counter()
            latency = t1 - t0
            memory = proc.memory_info().rss / 1024 ** 2

            metrics_df = pd.DataFrame([{
                "Mode": "Quantized",
                "Batch Size": batch_size,
                "Latency (s)": latency,
                "Throughput (texts/s)": len(texts_for_run) / max(latency, 1e-12),
                "Memory (MB)": memory
            }])

            st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            st.metric("Latency (s)", f"{latency:.4f}")
            st.metric("Throughput (texts/s)", f"{len(texts_for_run)/max(latency,1e-12):.1f}")
            st.metric("Memory (MB)", f"{memory:.1f}")
            st.dataframe(metrics_df)

        else:  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö
            # Original
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                orig = SentenceTransformer(model_id)
            t0 = time.perf_counter()
            orig_embs = orig.encode(texts_for_run, normalize_embeddings=True)
            t1 = time.perf_counter()
            orig_latency = t1 - t0

            # Quantized
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                quant = QuantModel(
                    model_id=quant_id,
                    source=quant_source,
                    model_dir=quant_dir,
                    tokenizer_name=tokenizer_name if tokenizer_name else None,
                    force_download=force_download
                )
            t0 = time.perf_counter()
            quant_embs = quant.encode(texts_for_run, normalize=True)
            t1 = time.perf_counter()
            quant_latency = t1 - t0

            # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ñ–æ—Ä–º (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
            O = np.asarray(orig_embs)
            Q = np.asarray(quant_embs)
            if O.ndim == 3:
                O = O.mean(axis=1)
            if Q.ndim == 3:
                Q = Q.mean(axis=1)
            if O.shape[1] != Q.shape[1]:
                m = min(O.shape[1], Q.shape[1])
                O = O[:, :m]
                Q = Q[:, :m]

            # –ö–æ—Å–∏–Ω—É—Å—ã –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–µ–∫—Å—Ç—É + —Å—Ä–µ–¥–Ω–µ–µ
            per_text_cos = cosine_batch(O, Q)
            avg_cos = float(per_text_cos.mean())

            # –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ (–±–µ–∑ –ø–∞–º—è—Ç–∏)
            metrics_df = pd.DataFrame([
                {
                    "Mode": "Original",
                    "Batch Size": batch_size,
                    "Latency (s)": orig_latency,
                    "Throughput (texts/s)": len(texts_for_run) / max(orig_latency, 1e-12),
                },
                {
                    "Mode": "Quantized",
                    "Batch Size": batch_size,
                    "Latency (s)": quant_latency,
                    "Throughput (texts/s)": len(texts_for_run) / max(quant_latency, 1e-12),
                },
            ])

            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏")
            st.dataframe(metrics_df)
            st.subheader("üéØ –ö–∞—á–µ—Å—Ç–≤–æ")
            st.write(f"–°—Ä–µ–¥–Ω—è—è cosine similarity (–ø–æ {len(per_text_cos)} —Ç–µ–∫—Å—Ç–∞–º): **{avg_cos:.4f}**")

        # –ö–Ω–æ–ø–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ CSV (–¥–ª—è –ª—é–±–æ–≥–æ —Ä–µ–∂–∏–º–∞)
        if metrics_df is not None:
            csv = metrics_df.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                data=csv,
                file_name="metrics.csv",
                mime="text/csv",
            )

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
